{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "SynthSR prediction\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print information\n",
    "print('\\n')\n",
    "print('SynthSR prediction')\n",
    "print('\\n')\n",
    "\n",
    "# python imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from argparse import ArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export PATH=/home/viscent/anaconda3/envs/synthsr/bin:$PATH\n",
    "# !export LD_LIBRARY_PATH=/home/viscent/anaconda3/envs/synthsr/lib:$LD_LIBRARY_PATH\n",
    "!export PATH=/usr/local/cuda-10.0/bin:$PATH\n",
    "!export LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64:$LD_LIBRARY_PATH\n",
    "# !export CUDA_VISIBLE_DEVICES=-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add main folder to python path and import SynthSR packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/viscent/hdd/viscent/SynthSR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "synthSR_home = '/home/viscent/hdd/viscent/SynthSR'\n",
    "print(synthSR_home)\n",
    "sys.path.append(synthSR_home)\n",
    "from ext.neuron import models as nrn_models\n",
    "from ext.lab2im import utils\n",
    "from ext.lab2im import edit_volumes\n",
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If you use this code, please the SynthSR paper in:\n",
    "https://github.com/BBillot/SynthSR/blob/master/bibtex.bib\n",
    "\n",
    "Copyright 2020 Benjamin Billot\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in\n",
    "compliance with the License. You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is\n",
    "distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
    "implied. See the License for the specific language governing permissions and limitations under the\n",
    "License.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# python imports\n",
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import models\n",
    "import keras.callbacks as KC\n",
    "from keras.optimizers import Adam\n",
    "from inspect import getmembers, isclass\n",
    "\n",
    "# project imports\n",
    "from SynthSR.brain_generator import BrainGenerator\n",
    "from SynthSR.metrics_model import metrics_model, IdentityLoss, add_seg_loss_to_model\n",
    "\n",
    "# third-party imports\n",
    "from ext.lab2im import utils\n",
    "from ext.lab2im import layers as l2i_layers\n",
    "from ext.neuron import layers as nrn_layers\n",
    "from ext.neuron import models as nrn_models\n",
    "\n",
    "\n",
    "def training(labels_dir,\n",
    "             model_dir,\n",
    "             prior_means,\n",
    "             prior_stds,\n",
    "             path_generation_labels,\n",
    "             segmentation_label_list=None,\n",
    "             segmentation_label_equivalency=None,\n",
    "             segmentation_model_file=None,\n",
    "             fs_header_segnet=False,\n",
    "             relative_weight_segmentation=0.25,\n",
    "             prior_distributions='normal',\n",
    "             images_dir=None,\n",
    "             path_generation_classes=None,\n",
    "             FS_sort=True,\n",
    "             batchsize=1,\n",
    "             input_channels=True,\n",
    "             output_channel=0,\n",
    "             target_res=None,\n",
    "             output_shape=None,\n",
    "             flipping=True,\n",
    "             padding_margin=None,\n",
    "             scaling_bounds=0.15,\n",
    "             rotation_bounds=15,\n",
    "             shearing_bounds=0.02,\n",
    "             translation_bounds=5,\n",
    "             nonlin_std=4.,\n",
    "             nonlin_shape_factor=0.03125,\n",
    "             simulate_registration_error=True,\n",
    "             data_res=None,\n",
    "             thickness=None,\n",
    "             randomise_res=None,\n",
    "             downsample=True,\n",
    "             blur_range=1.15,\n",
    "             build_reliability_maps=True,\n",
    "             bias_field_std=.3,\n",
    "             bias_shape_factor=0.03125,\n",
    "             n_levels=5,\n",
    "             nb_conv_per_level=2,\n",
    "             conv_size=3,\n",
    "             unet_feat_count=24,\n",
    "             feat_multiplier=2,\n",
    "             dropout=0,\n",
    "             activation='elu',\n",
    "             lr=1e-4,\n",
    "             lr_decay=0,\n",
    "             epochs=100,\n",
    "             steps_per_epoch=1000,\n",
    "             regression_metric='l1',\n",
    "             work_with_residual_channel=None,\n",
    "             loss_cropping=None,\n",
    "             checkpoint=None,\n",
    "             model_file_has_different_lhood_layer=False):\n",
    "    \"\"\"\n",
    "    This function trains a Unet to do slice imputation (and possibly synthesis) of MRI images with thick slices,\n",
    "    using synthetic scans and possibly real scans.\n",
    "\n",
    "    :param labels_dir: path of folder with all input label maps, or to a single label map (if only one training example)\n",
    "    :param model_dir: path of a directory where the models will be saved during training.\n",
    "    :param images_dir: directory with real images corresponding to the training label maps. These will be taken as\n",
    "    regression target. We recommend skull stripping them.\n",
    "\n",
    "    #---------------------------------------------- Generation parameters ----------------------------------------------\n",
    "    # label maps parameters\n",
    "    :param path_generation_labels: list of all possible label values in the input label maps.\n",
    "    Must be the path to a 1d numpy array, which should be organised as follows: background label first, then non-sided\n",
    "    labels (e.g. CSF, brainstem, etc.), then all the structures of the same hemisphere (can be left or right), and\n",
    "    finally all the corresponding contralateral structures (in the same order).\n",
    "    Example: [background_label, non-sided_1, ..., non-sided_n, left_1, ..., left_m, right_1, ..., right_m]\n",
    "    :param FS_sort: whether us FS_sort when creating list of labels with utils.get_list_labels. Default is True.\n",
    "\n",
    "    # output-related parameters\n",
    "    :param batchsize: (optional) number of images to generate per mini-batch. Default is 1.\n",
    "    :param input_channels: (optional) list of booleans indicating if each *synthetic* channel is going to be used as an\n",
    "    input for the downstream network. This also enables to know how many channels are going to be synthesised. Default\n",
    "    is True, which means generating 1 channel, and use it as input (either for plain SR with a synthetic target, or for\n",
    "    synthesis with a real target).\n",
    "    :param output_channel: (optional) a list with the indices of the output channels  (i.e. the synthetic regression\n",
    "    targets), if no real images were provided as regression target. Set to None if using real images as targets. Default\n",
    "    is the first channel (index 0).\n",
    "    :param target_res: (optional) target resolution of the generated images and corresponding label maps.\n",
    "    If None, the outputs will have the same resolution as the input label maps.\n",
    "    Can be a number (isotropic resolution), or the path to a 1d numpy array.\n",
    "    :param output_shape: (optional) desired shape of the output image, obtained by randomly cropping the generated image\n",
    "    Can be an integer (same size in all dimensions), a sequence, a 1d numpy array, or the path to a 1d numpy array.\n",
    "    Default is None, where no cropping is performed.\n",
    "\n",
    "    # GMM-sampling parameters\n",
    "    :param path_generation_classes: (optional) Indices regrouping generation labels into classes of same intensity\n",
    "    distribution. Regouped labels will thus share the same Gaussian when samling a new image. Should be the path to a 1d\n",
    "    numpy array with the same length as generation_labels. and contain values between 0 and K-1, where K is the total\n",
    "    number of classes. Default is all labels have different classes.\n",
    "    :param prior_distributions: (optional) type of distribution from which we sample the GMM parameters.\n",
    "    Can either be 'uniform', or 'normal'. Default is 'normal'.\n",
    "    :param prior_means: (optional) hyperparameters controlling the prior distributions of the GMM means. Because\n",
    "    these prior distributions are uniform or normal, they require by 2 hyperparameters. Can be a path to:\n",
    "    1) an array of shape (2, K), where K is the number of classes (K=len(generation_labels) if generation_classes is\n",
    "    not given). The mean of the Gaussian distribution associated to class k in [0, ...K-1] is sampled at each mini-batch\n",
    "    from U(prior_means[0,k], prior_means[1,k]) if prior_distributions is uniform, and from\n",
    "    N(prior_means[0,k], prior_means[1,k]) if prior_distributions is normal.\n",
    "    2) an array of shape (2*n_mod, K), where each block of two rows is associated to hyperparameters derived\n",
    "    from different modalities. In this case, if use_specific_stats_for_channel is False, we first randomly select a\n",
    "    modality from the n_mod possibilities, and we sample the GMM means like in 2).\n",
    "    If use_specific_stats_for_channel is True, each block of two rows correspond to a different channel\n",
    "    (n_mod=n_channels), thus we select the corresponding block to each channel rather than randomly drawing it.\n",
    "    Default is None, which corresponds all GMM means sampled from uniform distribution U(25, 225).\n",
    "    :param prior_stds: (optional) same as prior_means but for the standard deviations of the GMM.\n",
    "    Default is None, which corresponds to U(5, 25).\n",
    "\n",
    "    # spatial deformation parameters\n",
    "    :param flipping: (optional) whether to introduce right/left random flipping. Default is True.\n",
    "    :param  padding_margin: useful when cropping the loss but you are not using very large patches. Set to None for\n",
    "    determining it automatically from loss_cropping (not recommended if you use big volume sizes)\n",
    "    :param scaling_bounds: (optional) if apply_linear_trans is True, the scaling factor for each dimension is\n",
    "    sampled from a uniform distribution of predefined bounds. Can either be:\n",
    "    1) a number, in which case the scaling factor is independently sampled from the uniform distribution of bounds\n",
    "    (1-scaling_bounds, 1+scaling_bounds) for each dimension.\n",
    "    2) the path to a numpy array of shape (2, n_dims), in which case the scaling factor in dimension i is sampled from\n",
    "    the uniform distribution of bounds (scaling_bounds[0, i], scaling_bounds[1, i]) for the i-th dimension.\n",
    "    3) False, in which case scaling is completely turned off.\n",
    "    Default is scaling_bounds = 0.15 (case 1)\n",
    "    :param rotation_bounds: (optional) same as scaling bounds but for the rotation angle, except that for case 1 the\n",
    "    bounds are centred on 0 rather than 1, i.e. (0+rotation_bounds[i], 0-rotation_bounds[i]).\n",
    "    Default is rotation_bounds = 15.\n",
    "    :param shearing_bounds: (optional) same as scaling bounds. Default is shearing_bounds = 0.012.\n",
    "    :param translation_bounds: (optional) same as scaling bounds. Default is translation_bounds = False, but we\n",
    "    encourage using it when cropping is deactivated (i.e. when output_shape=None).\n",
    "    :param nonlin_std: (optional) Standard deviation of the normal distribution from which we sample the first\n",
    "    tensor for synthesising the deformation field. Set to 0 to completely turn the elastic deformation off.\n",
    "    :param nonlin_shape_factor: (optional) Ratio between the size of the input label maps and the size of the sampled\n",
    "    tensor for synthesising the elastic deformation field.\n",
    "    :param simulate_registration_error: (optional) whether to simulate registration errors between *synthetic* channels.\n",
    "    Can be a single value (same for all channels) or a list with one value per *synthetic* channel. For the latter,\n",
    "    the first value will automatically be reset to True since the first channel is used as reference. Default is True.\n",
    "\n",
    "    # blurring/resampling parameters\n",
    "    :param randomise_res: (optional) whether to mimic images that would have been 1) acquired at low resolution, and\n",
    "    2) resampled to high resolution. The low resolution is uniformly sampled at each minibatch from [1mm, 9mm].\n",
    "    In that process, the images generated by sampling the GMM are: 1) blurred at LR, 2) downsampled at LR, and\n",
    "    3) resampled at target_resolution.\n",
    "    :param data_res: (optional) specific acquisition resolution to mimic, as opposed to random resolution sampled when\n",
    "    randomis_res is True. This triggers a blurring which mimics the acquisition resolution, but downsampling is optional\n",
    "    (see param downsample). Default for data_res is None, where images are slighlty blurred. If the generated images are\n",
    "    uni-modal, data_res can be a number (isotropic acquisition resolution), a sequence, a 1d numpy array, or the path\n",
    "    to a 1d numy array. In the multi-modal case, it should be given as a umpy array (or a path) of size (n_mod, n_dims),\n",
    "    where each row is the acquisition resolution of the corresponding channel.\n",
    "    :param thickness: (optional) if data_res is provided, we can further specify the slice thickness of the low\n",
    "    resolution images to mimic. Must be provided in the same format as data_res. Default thickness = data_res.\n",
    "    :param downsample: (optional) whether to actually downsample the volume images to data_res after blurring.\n",
    "    Default is False, except when thickness is provided, and thickness < data_res.\n",
    "    :param blur_range: (optional) Randomise the standard deviation of the blurring kernels, (whether data_res is given\n",
    "    or not). At each mini_batch, the standard deviation of the blurring kernels are multiplied by a coefficient sampled\n",
    "    from a uniform distribution with bounds [1/blur_range, blur_range]. If None, no randomisation. Default is 1.15.\n",
    "    :param build_reliability_maps: set to True if you want to build soft masks indicating which voxels are\n",
    "    \"measured\" and which are interpolated\n",
    "\n",
    "    # bias field parameters\n",
    "    :param bias_field_std: (optional) If strictly positive, this triggers the corruption of synthesised images with a\n",
    "    bias field. This will only affect the input channels (i.e. not the synthetic regression target). The bias field is\n",
    "    obtained by sampling a first small tensor from a normal distribution, resizing it to full size, and rescaling it to\n",
    "    positive values by taking the voxel-wise exponential. bias_field_std designates the std dev of the normal\n",
    "    distribution from which we sample the first tensor. Set to 0 to completely deactivate biad field corruption.\n",
    "    :param bias_shape_factor: (optional) If bias_field_std is not False, this designates the ratio between the size of\n",
    "    the input label maps and the size of the first sampled tensor for synthesising the bias field.\n",
    "\n",
    "    # ------------------------------------------ UNet architecture parameters ------------------------------------------\n",
    "    :param n_levels: (optional) number of level for the Unet. Default is 5.\n",
    "    :param nb_conv_per_level: (optional) number of convolutional layers per level. Default is 2.\n",
    "    :param conv_size: (optional) size of the convolution kernels. Default is 2.\n",
    "    :param unet_feat_count: (optional) number of feature for the first layr of the Unet. Default is 24.\n",
    "    :param feat_multiplier: (optional) multiply the number of feature by this nummber at each new level. Default is 2.\n",
    "    :param dropout: (optional) probability of dropout for the Unet. Deafult is 0, where no dropout is applied.\n",
    "    :param activation: (optional) activation function. Can be 'elu', 'relu'.\n",
    "\n",
    "    # ----------------------------------------------- Training parameters ----------------------------------------------\n",
    "    :param lr: (optional) learning rate for the training. Default is 1e-4\n",
    "    :param lr_decay: (optional) learing rate decay. Default is 0, where no decay is applied.\n",
    "    :param epochs: (optional) number of epochs.\n",
    "    :param steps_per_epoch: (optional) number of steps per epoch. Default is 1000. Since no online validation is\n",
    "    possible, this is equivalent to the frequency at which the models are saved.\n",
    "    :param regression_metric: (optional) loss used in training. Can be 'l1' (default), 'l2', 'ssim', or 'laplace'\n",
    "    :param work_with_residual_channel: (optional) if you have a channel that is similar to the output (e.g., in\n",
    "    imputation), it is convenient to predict the residual, rather than the image from scratch. This parameter is a list\n",
    "    of indices of the synthetic channels you want to add the residual to (must have the same length as output_channels,\n",
    "    or have length equal to 1 if real images are used)\n",
    "    :param loss_cropping: (option)  to crop the posteriors when evaluating the loss function (specify the output size\n",
    "    Can be an int, or the path to a 1d numpy array.\n",
    "    :param checkpoint: (optional) path of an already saved model to load before starting the training.\n",
    "    :param model_file_has_different_lhood_layer: (optional) set to True if eg you're loading weights from a segmetation\n",
    "    (rather than SR/synthesis) net. Useful to use models pretrained with SynthSeg or different number of channels\n",
    "\n",
    "    # ----------------------------------------------- Regularize with pretrained segmentation CNN-----------------------\n",
    "    :param segmentation_model_file: (optional) h5 model file with the weights of the segmentation model. For now, we\n",
    "    assume a Unet architecture with the shame shape as the synthesis / SR Unet. Set to None not to use.\n",
    "    :param fs_header_segnet: set to True if the segmentation network expects data in FS orientation (rather than the\n",
    "    default diagonal voxel-to-ras matrix).\n",
    "    :param segmentation_label_list: (optional) npy/npz file with an array with the list of labels segmented by the Unet\n",
    "    :param segmentation_label_equivalency: (optional) npy/npz file with an array with as many elements as\n",
    "    segmentation_label_list, pinpointing to which generation labels the segmentation labels correspond (set to -1 if you\n",
    "    don't want to use a label in the loss). You can use this array e.g., to merge left and right structures, ignore\n",
    "    structures...\n",
    "    :param relative_weight_segmentation: (optional) relative weight of the Dice loss, compared with the image term loss\n",
    "    (eg., l1 or l2)\n",
    "    \"\"\"\n",
    "\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "        # 设置 GPU 显存占用为按需分配，增长式\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError as e:\n",
    "            # 异常处理\n",
    "            print(e)\n",
    "\n",
    "    n_channels = len(utils.reformat_to_list(input_channels))\n",
    "\n",
    "    # convert output_channel and work_with_residual_channel to lists\n",
    "    if output_channel is not None:\n",
    "        output_channel = list(utils.reformat_to_list(output_channel))\n",
    "        n_output_channels = len(output_channel)\n",
    "    else:\n",
    "        n_output_channels = 1\n",
    "\n",
    "    # various checks\n",
    "    if (images_dir is None) & (output_channel is None):\n",
    "        raise Exception('please provide a value for output_channel or image_dir')\n",
    "    elif (images_dir is not None) & (output_channel is not None):\n",
    "        raise Exception('please provide a value either for output_channel or image_dir, but not both at the same time')\n",
    "    if output_channel is not None:\n",
    "        if any(x >= n_channels for x in output_channel):\n",
    "            raise Exception('indices in output_channel cannot be greater than the total number of channels')\n",
    "\n",
    "    # check work_with_residual_channel\n",
    "    if work_with_residual_channel is not None:\n",
    "        work_with_residual_channel = utils.reformat_to_list(work_with_residual_channel)\n",
    "        if output_channel is not None:\n",
    "            if len(work_with_residual_channel) != len(output_channel):\n",
    "                raise Exception('The number or residual channels and output channels must be the same')\n",
    "\n",
    "        if any(x >= n_channels for x in work_with_residual_channel):\n",
    "            raise Exception('indices in work_with_residual_channel cannot be greater than the total number of channels')\n",
    "\n",
    "        if build_reliability_maps:  # consider indices of reliability maps\n",
    "            work_with_residual_channel = 2 * work_with_residual_channel\n",
    "\n",
    "    # get label lists\n",
    "    generation_labels, n_neutral_labels = utils.get_list_labels(label_list=path_generation_labels,\n",
    "                                                                labels_dir=labels_dir,\n",
    "                                                                FS_sort=FS_sort)\n",
    "\n",
    "    # prepare model folder\n",
    "    utils.mkdir(model_dir)\n",
    "\n",
    "    # compute padding_margin if needed\n",
    "    if loss_cropping == 0:\n",
    "        padding_margin = None\n",
    "    elif padding_margin is None:\n",
    "        padding_margin = utils.get_padding_margin(output_shape, loss_cropping)\n",
    "    print('Building gen...')\n",
    "    # instantiate BrainGenerator object\n",
    "    brain_generator = BrainGenerator(labels_dir=labels_dir,\n",
    "                                     images_dir=images_dir,\n",
    "                                     generation_labels=generation_labels,\n",
    "                                     n_neutral_labels=n_neutral_labels,\n",
    "                                     padding_margin=padding_margin,\n",
    "                                     batchsize=batchsize,\n",
    "                                     input_channels=input_channels,\n",
    "                                     output_channel=output_channel,\n",
    "                                     target_res=target_res,\n",
    "                                     output_shape=output_shape,\n",
    "                                     output_div_by_n=2 ** n_levels,\n",
    "                                     generation_classes=path_generation_classes,\n",
    "                                     prior_means=prior_means,\n",
    "                                     prior_stds=prior_stds,\n",
    "                                     prior_distributions=prior_distributions,\n",
    "                                     flipping=flipping,\n",
    "                                     scaling_bounds=scaling_bounds,\n",
    "                                     rotation_bounds=rotation_bounds,\n",
    "                                     shearing_bounds=shearing_bounds,\n",
    "                                     translation_bounds=translation_bounds,\n",
    "                                     nonlin_std=nonlin_std,\n",
    "                                     nonlin_shape_factor=nonlin_shape_factor,\n",
    "                                     simulate_registration_error=simulate_registration_error,\n",
    "                                     randomise_res=randomise_res,\n",
    "                                     data_res=data_res,\n",
    "                                     thickness=thickness,\n",
    "                                     downsample=downsample,\n",
    "                                     blur_range=blur_range,\n",
    "                                     build_reliability_maps=build_reliability_maps,\n",
    "                                     bias_field_std=bias_field_std,\n",
    "                                     bias_shape_factor=bias_shape_factor)\n",
    "    print('Gen Done!')\n",
    "    # transformation model\n",
    "    labels_to_image_model = brain_generator.labels_to_image_model\n",
    "    unet_input_shape = brain_generator.model_output_shape\n",
    "    print('Transform done!')\n",
    "    # prepare the Unet model\n",
    "    if regression_metric == 'laplace':\n",
    "        nb_labels_unet = 2 * n_output_channels\n",
    "    else:\n",
    "        nb_labels_unet = n_output_channels\n",
    "    print('Building Unet...')\n",
    "    model = nrn_models.unet(nb_features=unet_feat_count,\n",
    "                            input_shape=unet_input_shape,\n",
    "                            nb_levels=n_levels,\n",
    "                            conv_size=conv_size,\n",
    "                            nb_labels=nb_labels_unet,\n",
    "                            feat_mult=feat_multiplier,\n",
    "                            nb_conv_per_level=nb_conv_per_level,\n",
    "                            conv_dropout=dropout,\n",
    "                            final_pred_activation='linear',\n",
    "                            batch_norm=-1,\n",
    "                            activation=activation,\n",
    "                            input_model=labels_to_image_model)\n",
    "    min_model = nrn_models.unet(nb_features=unet_feat_count,\n",
    "                            input_shape=unet_input_shape,\n",
    "                            nb_levels=n_levels,\n",
    "                            conv_size=conv_size,\n",
    "                            nb_labels=nb_labels_unet,\n",
    "                            feat_mult=feat_multiplier,\n",
    "                            nb_conv_per_level=nb_conv_per_level,\n",
    "                            conv_dropout=dropout,\n",
    "                            final_pred_activation='linear',\n",
    "                            batch_norm=-1,\n",
    "                            activation=activation)\n",
    "    # unet_model = nrn_models.unet(nb_features=24,\n",
    "    #                             input_shape=[None, None, None, 1],\n",
    "    #                             nb_levels=5,\n",
    "    #                             conv_size=3,\n",
    "    #                             nb_labels=1,\n",
    "    #                             feat_mult=2,\n",
    "    #                             nb_conv_per_level=2,\n",
    "    #                             conv_dropout=0,\n",
    "    #                             final_pred_activation='linear',\n",
    "    #                             batch_norm=-1,\n",
    "    #                             activation='elu',\n",
    "    #                             input_model=None)\n",
    "    # print('Unet model loaded')\n",
    "    # unet_model.load_weights('/media/hdd/viscent/SynthSR/models/SynthSR_v10_210712.h5', by_name=True)\n",
    "    # input generator\n",
    "    input_generator = utils.build_training_generator(brain_generator.model_inputs_generator, batchsize)\n",
    "    # model\n",
    "    model = models.Model(model.inputs, model.output)\n",
    "    model = metrics_model(input_model=model,\n",
    "                          loss_cropping=loss_cropping,\n",
    "                          metrics=regression_metric,\n",
    "                          work_with_residual_channel=work_with_residual_channel)\n",
    "    return model, min_model, unet_input_shape\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/hdd/viscent/SynthSR/data/estimated_priors/prior_means.npy\n",
      "/media/hdd/viscent/SynthSR/data/estimated_priors/prior_stds.npy\n",
      "Building gen...\n",
      "/media/hdd/dhcp/dhcp_lores/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 15:42:39.706557: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2022-04-21 15:42:39.725971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-21 15:42:39.726062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: NVIDIA GeForce RTX 3090 major: 8 minor: 6 memoryClockRate(GHz): 1.8\n",
      "pciBusID: 0000:08:00.0\n",
      "2022-04-21 15:42:39.726123: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/viscent/software/Slicer-4.11/lib:/home/viscent/software/itksnap-3.8.0-qt4/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-11.6/lib64:\n",
      "2022-04-21 15:42:39.726160: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/viscent/software/Slicer-4.11/lib:/home/viscent/software/itksnap-3.8.0-qt4/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-11.6/lib64:\n",
      "2022-04-21 15:42:39.726195: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/viscent/software/Slicer-4.11/lib:/home/viscent/software/itksnap-3.8.0-qt4/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-11.6/lib64:\n",
      "2022-04-21 15:42:39.726227: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/viscent/software/Slicer-4.11/lib:/home/viscent/software/itksnap-3.8.0-qt4/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-11.6/lib64:\n",
      "2022-04-21 15:42:39.726259: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/viscent/software/Slicer-4.11/lib:/home/viscent/software/itksnap-3.8.0-qt4/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-11.6/lib64:\n",
      "2022-04-21 15:42:39.726290: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/viscent/software/Slicer-4.11/lib:/home/viscent/software/itksnap-3.8.0-qt4/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-11.6/lib64:\n",
      "2022-04-21 15:42:39.726321: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/viscent/software/Slicer-4.11/lib:/home/viscent/software/itksnap-3.8.0-qt4/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-11.6/lib64:\n",
      "2022-04-21 15:42:39.726325: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-04-21 15:42:40.020608: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2022-04-21 15:42:40.043303: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3393315000 Hz\n",
      "2022-04-21 15:42:40.044020: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557309722820 executing computations on platform Host. Devices:\n",
      "2022-04-21 15:42:40.044034: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n",
      "2022-04-21 15:42:40.101351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-21 15:42:40.101479: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557309491400 executing computations on platform CUDA. Devices:\n",
      "2022-04-21 15:42:40.101490: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2022-04-21 15:42:40.101556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-04-21 15:42:40.101560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Done!\n",
      "Transform done!\n",
      "Building Unet...\n",
      "Building enc...\n",
      "Building dec...\n",
      "dec built\n",
      "Building enc...\n",
      "Building dec...\n",
      "dec built\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This script shows how to call the training function. It re-uses the last example (6-SR_synthesis_synthetic), where\n",
    "we use T1 and T2 synthetic as input channels (at HR but simulating data acquired at LR) and synthetic HR T1 scans as\n",
    "regression target.\n",
    "\n",
    "\n",
    "If you use this code, please the SynthSR paper in:\n",
    "https://github.com/BBillot/SynthSR/blob/master/bibtex.bib\n",
    "\n",
    "Copyright 2020 Benjamin Billot\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in\n",
    "compliance with the License. You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is\n",
    "distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
    "implied. See the License for the specific language governing permissions and limitations under the\n",
    "License.\n",
    "\"\"\"\n",
    "import os\n",
    "from pathlib import Path\n",
    "home_dir = '/media/hdd/viscent'\n",
    "\n",
    "# change directory (to import SynthSR code) - NOT REQUIRED IF RUNNING FROM COMMAND LINE?\n",
    "#os.chdir(home_dir+'/Python/github/SynthSR')\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,os.path.join(home_dir,'SynthSR'))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data_dir = '/media/hdd/dhcp/dhcp_lores'\n",
    "\n",
    "# we have to specify a model dir, where the models will be saved after each epoch\n",
    "#model_dir = '../../data/generated_images/7-training'  # folder where they will be saved\n",
    "model_dir = os.path.join(home_dir,'SynthSR','models','all_to_t1')  # folder where they will be saved\n",
    "\n",
    "# we specify the Unet architecture\n",
    "n_levels = 5  # number of levels in the UNet\n",
    "nb_conv_per_level = 2  # number of convolution per level\n",
    "conv_size = 3  # size of the convolution kernels\n",
    "unet_feat_count = 24  # number of feature maps after the very first layer of the network\n",
    "# here we double the number of feature maps after each max-pooling operation. Incidentally, the number of features will\n",
    "# be halved after each upsampling step. Set to 1 to keep the number of feature maps constant throughout the network.\n",
    "feat_multiplier = 2\n",
    "dropout = 0  # We recommend not using dropout\n",
    "activation = 'elu'  # activation function\n",
    "\n",
    "# we now set the learning parameters\n",
    "learning_rate = 1e-4  # learning rate to apply\n",
    "lr_decay = 0   # here we do not use a decay. I fyou do, remember that it will be applied at each step !\n",
    "# An epoch is defined by a given number of steps (rather than the fact to have gone through all the training examples).\n",
    "# This choice is motivated by the fact that we typically have a small amount of data in medical imaging analysis.\n",
    "# At each step, we randomly select a training label map, generate the training data, run the input channels through the\n",
    "# network, compute the regression metric between the prediction and the regresion target, and finally backpropagate.\n",
    "# We set here the number of epochs and steps per epoch to low values, as this is just an example, but it would typically\n",
    "# be 200 epochs with 1,000 steps each.\n",
    "epochs = 200  # number of epochs\n",
    "steps_per_epoch = 4000  # number of steps per epoch\n",
    "regression_metric = 'l1'  # metric used to compute the loss function\n",
    "# In this example, the regression target and one of the input channels have the same contrast (T1 scan). Therefore, it\n",
    "# is easier to predict the residuals between the two. To do that we need to indicate the index of the input channel to\n",
    "# add the residuals to\n",
    "#work_with_residual_channel = 1\n",
    "work_with_residual_channel = 0 # changed to 0 (as scenario is not multimodal and hence has only one channel)\n",
    "\n",
    "# We now set the generation parameters, same as before\n",
    "\n",
    "# data paths\n",
    "#labels_folder = '../../data/labels'\n",
    "#images_folder = None\n",
    "labels_folder = data_dir+'/labels'  # 1 mm isotropic\n",
    "images_folder = data_dir+'/images_t1'   # 1 mm isotropic\n",
    "\n",
    "# # general parameters\n",
    "# input_channels = [False, True, True]  # specify which channel will be used as input channel for the network\n",
    "# output_channel = 0   # index corresponding to the regression target\n",
    "# target_res = None\n",
    "# output_shape = 128\n",
    "# copied from 1-SR_real_dhcp\n",
    "input_channels = [True, True]\n",
    "output_channel = None\n",
    "target_res = None\n",
    "output_shape = None\n",
    "\n",
    "# GMM-sampling parameters\n",
    "# generation_labels = '../../data/labels_classes_priors/generation_labels.npy'\n",
    "# generation_classes = '../../data/labels_classes_priors/generation_classes.npy'\n",
    "# prior_means_t1_hr = np.load('../../data/labels_classes_priors/prior_means_t1_hr.npy')\n",
    "# prior_means_t1_lr = np.load('../../data/labels_classes_priors/prior_means_t1_lr.npy')\n",
    "# prior_means_t2 = np.load('../../data/labels_classes_priors/prior_means_t2.npy')\n",
    "# prior_means = np.concatenate([prior_means_t1_hr, prior_means_t1_lr, prior_means_t2], axis=0)\n",
    "# prior_stds_t1_hr = np.load('../../data/labels_classes_priors/prior_stds_t1_hr.npy')\n",
    "# prior_stds_t1_lr = np.load('../../data/labels_classes_priors/prior_stds_t1_lr.npy')\n",
    "# prior_stds_t2 = np.load('../../data/labels_classes_priors/prior_stds_t2.npy')\n",
    "# prior_stds = np.concatenate([prior_stds_t1_hr, prior_stds_t1_lr, prior_stds_t2], axis=0)\n",
    "\n",
    "generation_labels = home_dir+'/SynthSR/data/labels_classes_priors/generation_labels.npy'\n",
    "generation_classes = home_dir+'/SynthSR/data/labels_classes_priors/generation_classes.npy'\n",
    "prior_means = home_dir+'/SynthSR/data/estimated_priors/prior_means.npy'\n",
    "prior_stds = home_dir+'/SynthSR/data/estimated_priors/prior_stds.npy'\n",
    "\n",
    "# augmentation parameters\n",
    "flipping = True\n",
    "scaling_bounds = 0.1\n",
    "rotation_bounds = 8\n",
    "shearing_bounds = 0.01\n",
    "translation_bounds = False\n",
    "nonlin_std = 2.\n",
    "bias_field_std = 0.2\n",
    "\n",
    "# blurring/downsampling parameters\n",
    "#data_res = np.array([[1., 1., 3.], [1., 4.5, 1.]])  # slice spacing for the input channels only\n",
    "#thickness = np.array([[1., 1., 3.], [1., 3., 1.]])\n",
    "# Hyperfine resolution\n",
    "data_res = np.array([1.5, 1.5, 5.])  # slice spacing i.e. resolution to mimic\n",
    "thickness = np.array([1.5, 1.5, 5.])  # slice thickness\n",
    "\n",
    "downsample = True\n",
    "build_reliability_maps = True\n",
    "blur_range = 1.15\n",
    "simulate_registration_error = True\n",
    "\n",
    "print(prior_means)\n",
    "print(prior_stds)\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "# launch training\n",
    "unet_model,min_model,input_shape=training(labels_folder,\n",
    "         model_dir,\n",
    "         prior_means,\n",
    "         prior_stds,\n",
    "         images_dir=images_folder,\n",
    "         path_generation_labels=generation_labels,\n",
    "         path_generation_classes=generation_classes,\n",
    "         batchsize=1,\n",
    "         input_channels=input_channels,\n",
    "         output_channel=output_channel,\n",
    "         target_res=target_res,\n",
    "         output_shape=output_shape,\n",
    "         flipping=flipping,\n",
    "         scaling_bounds=scaling_bounds,\n",
    "         rotation_bounds=rotation_bounds,\n",
    "         shearing_bounds=shearing_bounds,\n",
    "         translation_bounds=translation_bounds,\n",
    "         nonlin_std=nonlin_std,\n",
    "         simulate_registration_error=True,\n",
    "         data_res=data_res,\n",
    "         thickness=thickness,\n",
    "         downsample=downsample,\n",
    "         randomise_res=False,\n",
    "         blur_range=blur_range,\n",
    "         build_reliability_maps=build_reliability_maps,\n",
    "         bias_field_std=bias_field_std,\n",
    "         n_levels=n_levels,\n",
    "         nb_conv_per_level=nb_conv_per_level,\n",
    "         conv_size=conv_size,\n",
    "         unet_feat_count=unet_feat_count,\n",
    "         feat_multiplier=feat_multiplier,\n",
    "         dropout=dropout,\n",
    "         activation=activation,\n",
    "         lr=learning_rate,\n",
    "         lr_decay=lr_decay,\n",
    "         epochs=epochs,\n",
    "         steps_per_epoch=steps_per_epoch,\n",
    "         regression_metric=regression_metric,\n",
    "         work_with_residual_channel=work_with_residual_channel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model.load_weights(os.path.join(synthSR_home, '/home/viscent/hdd/viscent/SynthSR/models/all_to_t1/200.h5'), by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils.vis_utils    import plot_model\n",
    "# import pydot\n",
    "# plot_model(unet_model, to_file=os.path.join(synthSR_home, 'models/all_to_t1/model.png'), show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers as KL\n",
    "from keras.models import Model\n",
    "input_tensor = KL.Input(shape=input_shape, name=None)\n",
    "last_tensor = input_tensor\n",
    "restored_w = []\n",
    "for l in min_model.layers:\n",
    "    name=l.name\n",
    "    if name.startswith('unet_input'):\n",
    "        continue\n",
    "    restored_w.extend(unet_model.get_layer(name).get_weights())\n",
    "min_model.set_weights(restored_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare list of images to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_images = '/media/hdd/viscent/dHCP/synthsr_data/images_t1_skull_1mm'\n",
    "basename = os.path.basename(path_images)\n",
    "path_predictions = '/media/hdd/viscent/SynthSR/results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ('.nii.gz' not in basename) & ('.nii' not in basename) & ('.mgz' not in basename) & ('.npz' not in basename):\n",
    "    if os.path.isfile(path_images):\n",
    "        raise Exception('extension not supported for %s, only use: nii.gz, .nii, .mgz, or .npz' % path_images)\n",
    "    images_to_segment = utils.list_images_in_folder(path_images)\n",
    "    utils.mkdir(path_predictions)\n",
    "    path_predictions = [os.path.join(path_predictions, os.path.basename(image)).replace('.nii', '_SynthSR.nii') for image in\n",
    "                   images_to_segment]\n",
    "    path_predictions = [seg_path.replace('.mgz', '_SynthSR.mgz') for seg_path in path_predictions]\n",
    "    path_predictions = [seg_path.replace('.npz', '_SynthSR.npz') for seg_path in path_predictions]\n",
    "\n",
    "else:\n",
    "    assert os.path.isfile(path_images), \"files does not exist: %s \" \\\n",
    "                                        \"\\nplease make sure the path and the extension are correct\" % path_images\n",
    "    images_to_segment = [path_images]\n",
    "    path_predictions = [path_predictions]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the inbox model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building enc...\n",
      "Building dec...\n",
      "dec built\n",
      "  /home/viscent/hdd/viscent/SynthSR/results/CHILD_T1_hf_brain_resized_SynthSR.nii.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenJDK 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0\n",
      "OpenJDK 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Detected existing ImageJ; passing arguments along\n"
     ]
    }
   ],
   "source": [
    "import SimpleITK as sitk\n",
    "# Build Unet and load weights\n",
    "unet_model = nrn_models.unet(nb_features=24,\n",
    "                             input_shape=[None, None, None, 1],\n",
    "                             nb_levels=5,\n",
    "                             conv_size=3,\n",
    "                             nb_labels=1,\n",
    "                             feat_mult=2,\n",
    "                             nb_conv_per_level=2,\n",
    "                             conv_dropout=0,\n",
    "                             final_pred_activation='linear',\n",
    "                             batch_norm=-1,\n",
    "                             activation='elu',\n",
    "                             input_model=None)\n",
    "\n",
    "unet_model.load_weights(os.path.join(synthSR_home, 'models/SynthSR_v10_210712.h5'), by_name=True)\n",
    "print(os.getenv('SITK_SHOW_COMMAND'))\n",
    "path_image = '/home/viscent/hdd/viscent/SynthSR/results/CHILD_T1_hf_brain_resized_SynthSR.nii.gz'\n",
    "path_prediction = '/home/viscent/hdd/viscent/SynthSR/results/CHILD_T1_hf_brain_resized_SynthSR_pred_200.nii.gz'\n",
    "print('  ' + path_image)\n",
    "S=sitk.ReadImage(path_image)\n",
    "S=sitk.GetArrayFromImage(S)\n",
    "S=skimage.transform.resize(S,(96, 128, 128))\n",
    "S=np.expand_dims(S,axis=3)\n",
    "S=np.expand_dims(S,axis=0)\n",
    "output = unet_model.predict(S)\n",
    "output = np.squeeze(output)\n",
    "sitk.Show(sitk.GetImageFromArray(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building enc...\n",
      "Building dec...\n",
      "dec built\n",
      "/home/viscent/Downloads/Fiji.app/ImageJ-linux64\n",
      "  /home/viscent/hdd/viscent/SynthSR/results/CHILD_T1_hf_brain_resized_SynthSR.nii.gz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected unet_input to have shape (None, None, None, 2) but got array with shape (96, 128, 128, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_198305/1649172013.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munet_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0msitk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mShow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msitk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetImageFromArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/synthsr/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/synthsr/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/synthsr/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected unet_input to have shape (None, None, None, 2) but got array with shape (96, 128, 128, 1)"
     ]
    }
   ],
   "source": [
    "unet_model = nrn_models.unet(nb_features=24,\n",
    "                             input_shape=[None,None,None,2],\n",
    "                             nb_levels=5,\n",
    "                             conv_size=3,\n",
    "                             nb_labels=1,\n",
    "                             feat_mult=2,\n",
    "                             dilation_rate_mult=1,\n",
    "                             nb_conv_per_level=2,\n",
    "                             conv_dropout=False,\n",
    "                             final_pred_activation='linear',\n",
    "                             batch_norm=-1,\n",
    "                             input_model=None)\n",
    "\n",
    "unet_model.load_weights(os.path.join(synthSR_home, 'models/SynthSR_v10_210712_hyperfine.h5'), by_name=True)\n",
    "print(os.getenv('SITK_SHOW_COMMAND'))\n",
    "path_image_t1 = '/home/viscent/hdd/viscent/SynthSR/results/CHILD_T1_hf_brain_resized_SynthSR.nii.gz'\n",
    "path_image_t2 = '/home/viscent/hdd/viscent/SynthSR/results/CHILD_T2_hf_brain_resized_SynthSR.nii.gz'\n",
    "S_t1=sitk.ReadImage(path_image_t1)\n",
    "S_t2=sitk.ReadImage(path_image_t2)\n",
    "S=np.stack(sitk.GetArrayFromImage(S_t1),sitk.GetArrayFromImage(S_t2),axis=3)\n",
    "S=skimage.transform.resize(S,(96, 128, 128))\n",
    "S=np.expand_dims(S,axis=3)\n",
    "S=np.expand_dims(S,axis=0)\n",
    "output = unet_model.predict(S)\n",
    "output = np.squeeze(output)\n",
    "sitk.Show(sitk.GetImageFromArray(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/viscent/Downloads/Fiji.app/ImageJ-linux64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenJDK 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0\n",
      "OpenJDK 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release\n"
     ]
    }
   ],
   "source": [
    "os.environ['SITK_SHOW_COMMAND']='/home/viscent/Downloads/Fiji.app/ImageJ-linux64'\n",
    "\n",
    "!echo \"$SITK_SHOW_COMMAND\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/viscent/Downloads/Fiji.app/ImageJ-linux64\n",
      "  /home/viscent/hdd/viscent/SynthSR/results/CHILD_T1_hf_brain_resized_SynthSR.nii.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenJDK 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0\n",
      "OpenJDK 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Detected existing ImageJ; passing arguments along\n"
     ]
    }
   ],
   "source": [
    "import skimage\n",
    "import SimpleITK as sitk\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "print(os.getenv('SITK_SHOW_COMMAND'))\n",
    "path_image = '/home/viscent/hdd/viscent/SynthSR/results/CHILD_T1_hf_brain_resized_SynthSR.nii.gz'\n",
    "path_prediction = '/home/viscent/hdd/viscent/SynthSR/results/CHILD_T1_hf_brain_resized_SynthSR_pred_200.nii.gz'\n",
    "print('  ' + path_image)\n",
    "S=sitk.ReadImage(path_image)\n",
    "S=sitk.GetArrayFromImage(S)\n",
    "S=skimage.transform.resize(S,(96, 128, 128))\n",
    "S=np.expand_dims(S,axis=3)\n",
    "S=np.expand_dims(S,axis=0)\n",
    "output = unet_model.predict(S)\n",
    "output = np.squeeze(output)\n",
    "sitk.Show(sitk.GetImageFromArray(output))\n",
    "sitk.WriteImage(sitk.GetImageFromArray(output), path_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Debug: In /tmp/SimpleITK/Code/IO/src/sitkImageViewer.cxx, line 495: ExecuteCommand: '/home/viscent/Downloads/Fiji.app/ImageJ-linux64' '/tmp/TempFile-198305-2.mha' \n",
      "\n",
      "\n",
      "Debug: In /tmp/SimpleITK/Code/IO/src/sitkImageViewer.cxx, line 536: Normal process exit.  exitValue = 0\n",
      "\n",
      "Debug: In /tmp/SimpleITK/Code/IO/src/sitkImageViewer.cxx, line 576: Done.  Deleting process.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sitk.Show(sitk.GetImageFromArray(S[...,0].squeeze()),debugOn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenJDK 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0\n",
      "OpenJDK 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Detected existing ImageJ; passing arguments along\n",
      "[INFO] Detected existing ImageJ; passing arguments along\n",
      "[INFO] Reading available sites from https://imagej.net/\n"
     ]
    }
   ],
   "source": [
    "import SimpleITK as sitk\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "input = os.path.join(path_images,'sub-CC00051XX02_ses-7702_T1w_brain_1mm.nii.gz')\n",
    "output = os.path.join('/media/hdd/viscent/SynthSR/results','sub-CC00051XX02_ses-7702_T1w_brain_1mm_SynthSR.nii.gz')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "569e3690921c7d6c62459c16ec8c5627002794f98ad6d34d582a2d4c13798947"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('synthsr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
